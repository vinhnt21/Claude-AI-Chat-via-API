import streamlit as st
import time
from typing import List, Dict
import json
from datetime import datetime

from llm_handler_anthropic import anthropic_handler, MODELS
from config import (
    PAGE_TITLE, PAGE_ICON, DEFAULT_MODEL, DEFAULT_MAX_TOKENS, 
    DEFAULT_BUDGET_TOKENS, CHAT_INPUT_PLACEHOLDER, MAX_HISTORY_LENGTH,
    DEFAULT_SYSTEM_PROMPT, DEBUG
)

# Streamlit page configuration
st.set_page_config(
    page_title=PAGE_TITLE,
    page_icon=PAGE_ICON,
    layout="wide",
    initial_sidebar_state="expanded"
)

def initialize_session_state():
    """Khá»Ÿi táº¡o session state"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "model_settings" not in st.session_state:
        st.session_state.model_settings = {
            "model": DEFAULT_MODEL,
            "max_tokens": DEFAULT_MAX_TOKENS,
            "thinking": False,
            "budget_tokens": DEFAULT_BUDGET_TOKENS,
            "temperature": 0.7,
            "system_prompt": DEFAULT_SYSTEM_PROMPT,
            "use_streaming": True
        }
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Initialize sync flags Ä‘á»ƒ trÃ¡nh infinite loop
    if "sync_flags" not in st.session_state:
        st.session_state.sync_flags = {
            "last_thinking_state": False
        }



def handle_thinking_temperature_sync(thinking_enabled, current_thinking):
    """Xá»­ lÃ½ Ä‘á»“ng bá»™ temperature khi thinking mode thay Ä‘á»•i"""
    # Kiá»ƒm tra xem thinking state cÃ³ thay Ä‘á»•i khÃ´ng
    if current_thinking != st.session_state.sync_flags["last_thinking_state"]:
        st.session_state.sync_flags["last_thinking_state"] = current_thinking
        
        if current_thinking:
            # LÆ°u temperature hiá»‡n táº¡i trÆ°á»›c khi set = 1.0
            if "saved_temperature" not in st.session_state:
                st.session_state.saved_temperature = st.session_state.model_settings["temperature"]
            st.session_state.model_settings["temperature"] = 1.0
        else:
            # KhÃ´i phá»¥c temperature Ä‘Ã£ lÆ°u
            if "saved_temperature" in st.session_state:
                st.session_state.model_settings["temperature"] = st.session_state.saved_temperature

def render_sidebar():
    """Render sidebar vá»›i cÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh"""
    with st.sidebar:
        st.header("âš™ï¸ Cáº¥u hÃ¬nh")
        
        # Model Selection
        st.subheader("ğŸ¤– Chá»n Model")
        
        model_options = list(MODELS.keys())
        model_labels = [anthropic_handler.format_model_display(model) for model in model_options]
        
        selected_model_index = model_options.index(st.session_state.model_settings["model"]) if st.session_state.model_settings["model"] in model_options else 0
        
        selected_model = st.selectbox(
            "Model:",
            options=model_options,
            index=selected_model_index,
            format_func=lambda x: anthropic_handler.format_model_display(x),
            key="model_selector"
        )
        
        st.session_state.model_settings["model"] = selected_model
        
        # Model Information
        model_info = anthropic_handler.get_model_info(selected_model)
        if model_info:
            with st.expander("â„¹ï¸ ThÃ´ng tin Model", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**Input:** {model_info['price']['input']}")
                    st.write(f"**Context:** {model_info['context_window']}")
                with col2:
                    st.write(f"**Output:** {model_info['price']['output']}")
                    st.write(f"**Max Output:** {model_info['max_output']}")
        
        st.divider()
        
        # Parameters
        st.subheader("ğŸ›ï¸ Tham sá»‘")
        
        # Max Tokens
        max_tokens = st.number_input(
            "Max Tokens:",
            min_value=100,
            max_value=50000,
            value=st.session_state.model_settings["max_tokens"],
            step=100,
            help="Sá»‘ token tá»‘i Ä‘a cho response"
        )
        st.session_state.model_settings["max_tokens"] = max_tokens
        
        # Temperature vá»›i Ä‘á»“ng bá»™ thinking mode
        thinking_enabled = model_info.get("extended_thinking", False) if model_info else False
        
        # Extended Thinking checkbox (Ä‘áº·t trÆ°á»›c temperature Ä‘á»ƒ xá»­ lÃ½ Ä‘á»“ng bá»™)
        if thinking_enabled:
            enable_thinking = st.checkbox(
                "ğŸ§  Báº­t Extended Thinking",
                value=st.session_state.model_settings["thinking"],
                key="thinking_checkbox",
                help="Cho phÃ©p model suy nghÄ© trÆ°á»›c khi tráº£ lá»i"
            )
            st.session_state.model_settings["thinking"] = enable_thinking
        else:
            st.session_state.model_settings["thinking"] = False
            
        current_thinking = st.session_state.model_settings["thinking"] and thinking_enabled
        
        # Xá»­ lÃ½ Ä‘á»“ng bá»™ thinking vÃ  temperature
        handle_thinking_temperature_sync(thinking_enabled, current_thinking)
        
        if current_thinking:
            # Khi thinking enabled, temperature cá»‘ Ä‘á»‹nh = 1.0
            st.markdown("**Temperature:** *1.0 (cá»‘ Ä‘á»‹nh khi thinking enabled)*")
            st.info("âš ï¸ Temperature Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh á»Ÿ 1.0 khi Extended Thinking Ä‘Æ°á»£c báº­t")
            # Hiá»ƒn thá»‹ temperature Ä‘Ã£ lÆ°u
            if "saved_temperature" in st.session_state:
                st.caption(f"ğŸ’¾ Temperature Ä‘Ã£ lÆ°u: {st.session_state.saved_temperature}")
        else:
            # Cho phÃ©p Ä‘iá»u chá»‰nh temperature
            temperature = st.number_input(
                "Temperature:",
                min_value=0.0,
                max_value=1.0,
                value=st.session_state.model_settings["temperature"],
                step=0.1,
                format="%.1f",
                help="Má»©c Ä‘á»™ sÃ¡ng táº¡o (0 = conservative, 1 = creative)"
            )
            st.session_state.model_settings["temperature"] = temperature
        
        # Extended Thinking Budget Tokens
        if thinking_enabled and st.session_state.model_settings["thinking"]:
            budget_tokens = st.number_input(
                "Budget Tokens cho Thinking:",
                min_value=1024,  # Minimum theo API
                max_value=100000,
                value=max(st.session_state.model_settings["budget_tokens"], 1024),
                step=1000,
                help="Sá»‘ tokens cho quÃ¡ trÃ¬nh thinking"
            )
            st.session_state.model_settings["budget_tokens"] = budget_tokens
            
            # Validation: budget_tokens pháº£i < max_tokens
            current_budget = st.session_state.model_settings["budget_tokens"]
            current_max = st.session_state.model_settings["max_tokens"]
            
            if current_budget >= current_max:
                st.error(f"âš ï¸ Budget tokens ({current_budget}) pháº£i nhá» hÆ¡n Max tokens ({current_max})")
                # Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh
                adjusted_budget = max(1024, current_max - 1000)
                st.session_state.model_settings["budget_tokens"] = adjusted_budget
                st.info(f"âœ… ÄÃ£ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh Budget tokens thÃ nh {adjusted_budget}")
                st.rerun()  # Refresh Ä‘á»ƒ cáº­p nháº­t UI
            
            # Warning náº¿u quÃ¡ gáº§n
            elif current_budget >= current_max * 0.8:
                st.warning("âš ï¸ Budget tokens quÃ¡ gáº§n Max tokens, cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n cháº¥t lÆ°á»£ng response")
        
        if not thinking_enabled:
            st.info("âš ï¸ Model nÃ y khÃ´ng há»— trá»£ Extended Thinking")
        
        st.divider()
        
        # System Prompt
        st.subheader("ğŸ“ System Prompt")
        system_prompt = st.text_area(
            "System Prompt:",
            value=st.session_state.model_settings["system_prompt"],
            height=100,
            placeholder="Nháº­p system prompt Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng AI...",
            help="HÆ°á»›ng dáº«n cho AI vá» cÃ¡ch pháº£n há»“i"
        )
        st.session_state.model_settings["system_prompt"] = system_prompt
        
        st.divider()
        
        # Streaming
        st.subheader("ğŸš€ TÃ¹y chá»n khÃ¡c")
        use_streaming = st.checkbox(
            "Báº­t Streaming",
            value=st.session_state.model_settings["use_streaming"],
            help="Hiá»ƒn thá»‹ response theo thá»i gian thá»±c"
        )
        st.session_state.model_settings["use_streaming"] = use_streaming
        
        # Debug mode
        if DEBUG:
            st.subheader("ğŸ› Debug")
            if st.button("Hiá»ƒn thá»‹ Session State"):
                st.json(dict(st.session_state))
        
        st.divider()
        
        # API Rules Information
        st.subheader("ğŸ“‹ Quy táº¯c API")
        with st.expander("â„¹ï¸ ThÃ´ng tin quan trá»ng", expanded=False):
            st.markdown("""
            **Extended Thinking Rules:**
            - Temperature pháº£i = 1.0 khi thinking enabled
            - Budget tokens â‰¥ 1024 (minimum)
            - Max tokens > Budget tokens
            - Chá»‰ model cÃ³ há»— trá»£ má»›i cÃ³ thá»ƒ dÃ¹ng thinking
            
            **Token Guidelines:**
            - Sá»­ dá»¥ng input number Ä‘á»ƒ nháº­p giÃ¡ trá»‹ chÃ­nh xÃ¡c
            - Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh náº¿u khÃ´ng há»£p lá»‡
            - Giao diá»‡n tá»‘i Æ°u Ä‘á»ƒ giáº£m reload trang
            """)
        
        st.divider()
        
        # Chat Management
        st.subheader("ğŸ’¬ Quáº£n lÃ½ Chat")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ XÃ³a Chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()
        
        with col2:
            if st.button("ğŸ’¾ LÆ°u Chat", use_container_width=True):
                save_chat_history()
        
        # Chat Statistics
        if st.session_state.messages:
            st.subheader("ğŸ“Š Thá»‘ng kÃª")
            total_messages = len(st.session_state.messages)
            user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
            assistant_messages = len([m for m in st.session_state.messages if m["role"] == "assistant"])
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Tá»•ng tin nháº¯n", total_messages)
            with col2:
                st.metric("Cá»§a báº¡n", user_messages)

def save_chat_history():
    """LÆ°u lá»‹ch sá»­ chat"""
    if st.session_state.messages:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"chat_history_{timestamp}.json"
        
        chat_data = {
            "timestamp": timestamp,
            "model_settings": st.session_state.model_settings,
            "messages": st.session_state.messages
        }
        
        # Trong mÃ´i trÆ°á»ng thá»±c táº¿, báº¡n cÃ³ thá»ƒ lÆ°u vÃ o file hoáº·c database
        st.session_state.chat_history.append(chat_data)
        st.success(f"âœ… ÄÃ£ lÆ°u chat history: {filename}")

def render_chat_interface():
    """Render giao diá»‡n chat chÃ­nh"""
    st.title(f"{PAGE_ICON} {PAGE_TITLE}")
    
    # Hiá»ƒn thá»‹ thÃ´ng tin model hiá»‡n táº¡i
    current_model = st.session_state.model_settings["model"]
    model_display = anthropic_handler.format_model_display(current_model)
    
    st.info(f"ğŸ¤– Äang sá»­ dá»¥ng: **{model_display}** | "
           f"Streaming: {'âœ…' if st.session_state.model_settings['use_streaming'] else 'âŒ'} | "
           f"Thinking: {'ğŸ§ ' if st.session_state.model_settings['thinking'] else 'âŒ'}")
    
    # Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input tá»« user
    if prompt := st.chat_input(CHAT_INPUT_PLACEHOLDER):
        # Giá»›i háº¡n Ä‘á»™ dÃ i lá»‹ch sá»­
        if len(st.session_state.messages) >= MAX_HISTORY_LENGTH:
            st.session_state.messages = st.session_state.messages[-(MAX_HISTORY_LENGTH-2):]
        
        # ThÃªm message cá»§a user
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Táº¡o response tá»« assistant
        with st.chat_message("assistant"):
            generate_response()

def sync_validated_parameters(validated_params):
    """Äá»“ng bá»™ parameters Ä‘Ã£ Ä‘Æ°á»£c validate trá»Ÿ láº¡i session state"""
    st.session_state.model_settings["max_tokens"] = validated_params["max_tokens"]
    st.session_state.model_settings["budget_tokens"] = validated_params["budget_tokens"] 
    st.session_state.model_settings["temperature"] = validated_params["temperature"]

def generate_response():
    """Táº¡o response tá»« AI"""
    settings = st.session_state.model_settings
    
    try:
        # Validate parameters trÆ°á»›c khi gá»i API
        validated = anthropic_handler.validate_and_fix_parameters(
            settings["model"],
            settings["max_tokens"],
            settings["budget_tokens"],
            settings["temperature"],
            settings["thinking"]
        )
        
        # Äá»“ng bá»™ parameters Ä‘Ã£ Ä‘Æ°á»£c validate
        sync_validated_parameters(validated)
        
        # Hiá»ƒn thá»‹ warnings náº¿u cÃ³
        if validated["warnings"]:
            warning_container = st.container()
            with warning_container:
                for warning in validated["warnings"]:
                    st.warning(f"âš ï¸ {warning}")
            time.sleep(1)  # Cho user Ä‘á»c warnings
        
        if settings["use_streaming"]:
            # Streaming response
            response_placeholder = st.empty()
            full_response = ""
            
            with st.spinner("ğŸ¤” Äang suy nghÄ©..."):
                for chunk in anthropic_handler.stream_response(
                    model=settings["model"],
                    messages=st.session_state.messages,
                    system_prompt=settings["system_prompt"] if settings["system_prompt"].strip() else None,
                    max_tokens=validated["max_tokens"],
                    thinking=settings["thinking"],
                    budget_tokens=validated["budget_tokens"],
                    temperature=validated["temperature"]
                ):
                    full_response += chunk
                    # ThÃªm cursor effect
                    response_placeholder.markdown(full_response + "â–Œ")
                    time.sleep(0.01)
            
            # Hiá»ƒn thá»‹ response cuá»‘i cÃ¹ng
            response_placeholder.markdown(full_response)
            
        else:
            # Non-streaming response
            with st.spinner("ğŸ¤” Äang táº¡o pháº£n há»“i..."):
                full_response = anthropic_handler.get_response(
                    model=settings["model"],
                    messages=st.session_state.messages,
                    system_prompt=settings["system_prompt"] if settings["system_prompt"].strip() else None,
                    max_tokens=validated["max_tokens"],
                    thinking=settings["thinking"],
                    budget_tokens=validated["budget_tokens"],
                    temperature=validated["temperature"]
                )
            st.markdown(full_response)
        
        # ThÃªm response vÃ o session state
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        
    except Exception as e:
        st.error(f"âŒ Lá»—i khi táº¡o response: {str(e)}")

def render_welcome_message():
    """Hiá»ƒn thá»‹ thÃ´ng bÃ¡o chÃ o má»«ng khi chÆ°a cÃ³ tin nháº¯n"""
    if not st.session_state.messages:
        st.markdown("""
        ### ğŸ‘‹ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i Claude AI Chat!
        
        **HÆ°á»›ng dáº«n sá»­ dá»¥ng:**
        - ğŸ¤– Chá»n model phÃ¹ há»£p trong sidebar
        - ğŸ›ï¸ Äiá»u chá»‰nh cÃ¡c tham sá»‘ theo nhu cáº§u (slider hoáº·c nháº­p trá»±c tiáº¿p)
        - ğŸ§  Báº­t Extended Thinking Ä‘á»ƒ xem quÃ¡ trÃ¬nh suy nghÄ© cá»§a AI
        - ğŸ“ Sá»­ dá»¥ng System Prompt Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng AI
        - ğŸš€ Báº­t Streaming Ä‘á»ƒ xem response theo thá»i gian thá»±c
        
        **TÃ­nh nÄƒng Ä‘Æ°á»£c cáº£i thiá»‡n:**
        - âœ… **Smart Validation**: Tá»± Ä‘á»™ng kiá»ƒm tra vÃ  Ä‘iá»u chá»‰nh parameters
        - ğŸšï¸ **Smart Temperature**: Tá»± Ä‘á»™ng cá»‘ Ä‘á»‹nh/khÃ´i phá»¥c khi báº­t/táº¯t thinking
        - ğŸ§  **Thinking Mode**: LÆ°u vÃ  khÃ´i phá»¥c temperature khi chuyá»ƒn Ä‘á»•i
        - âš¡ **Streamlined UI**: Chá»‰ sá»­ dá»¥ng input Ä‘á»ƒ trÃ¡nh reload nhiá»u láº§n
        - ğŸ“Š **Visual Warnings**: Hiá»ƒn thá»‹ cáº£nh bÃ¡o validation trá»±c quan
        
        **CÃ¡ch sá»­ dá»¥ng:**
        - ğŸ”¢ **Input Fields**: Nháº­p giÃ¡ trá»‹ trá»±c tiáº¿p cho cÃ¡c tham sá»‘
        - ğŸ’¾ **Auto Save**: Temperature Ä‘Æ°á»£c lÆ°u khi báº­t thinking vÃ  khÃ´i phá»¥c khi táº¯t
        - âš ï¸ **Smart Validation**: Há»‡ thá»‘ng tá»± Ä‘á»™ng kiá»ƒm tra vÃ  Ä‘iá»u chá»‰nh náº¿u cáº§n
        
        **Báº¯t Ä‘áº§u trÃ² chuyá»‡n báº±ng cÃ¡ch nháº­p tin nháº¯n á»Ÿ bÃªn dÆ°á»›i!** ğŸ‘‡
        
        *LÆ°u Ã½: Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ Ä‘á»ƒ tuÃ¢n thá»§ quy táº¯c API cá»§a Anthropic. Giao diá»‡n Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u Ä‘á»ƒ giáº£m reload trang.*
        """)

def main():
    """HÃ m chÃ­nh"""
    # Khá»Ÿi táº¡o session state
    initialize_session_state()
    
    # Render sidebar
    render_sidebar()
    
    # Render chat interface
    render_chat_interface()
    
    # Hiá»ƒn thá»‹ welcome message náº¿u chÆ°a cÃ³ tin nháº¯n
    render_welcome_message()

if __name__ == "__main__":
    main()