import streamlit as st
import time
from typing import List, Dict
import json
from datetime import datetime

from llm_handler_anthropic import anthropic_handler, MODELS
from config import (
    PAGE_TITLE, PAGE_ICON, DEFAULT_MODEL, DEFAULT_MAX_TOKENS, 
    DEFAULT_BUDGET_TOKENS, CHAT_INPUT_PLACEHOLDER, MAX_HISTORY_LENGTH,
    DEFAULT_SYSTEM_PROMPT, DEBUG
)

# Streamlit page configuration
st.set_page_config(
    page_title=PAGE_TITLE,
    page_icon=PAGE_ICON,
    layout="wide",
    initial_sidebar_state="expanded"
)

def initialize_session_state():
    """Kh·ªüi t·∫°o session state"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "model_settings" not in st.session_state:
        st.session_state.model_settings = {
            "model": DEFAULT_MODEL,
            "max_tokens": DEFAULT_MAX_TOKENS,
            "thinking": False,
            "budget_tokens": DEFAULT_BUDGET_TOKENS,
            "temperature": 0.7,
            "system_prompt": DEFAULT_SYSTEM_PROMPT,
            "use_streaming": True
        }
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Initialize sync flags ƒë·ªÉ tr√°nh infinite loop
    if "sync_flags" not in st.session_state:
        st.session_state.sync_flags = {
            "last_thinking_state": False
        }



def handle_thinking_temperature_sync(thinking_enabled, current_thinking):
    """X·ª≠ l√Ω ƒë·ªìng b·ªô temperature khi thinking mode thay ƒë·ªïi"""
    # Ki·ªÉm tra xem thinking state c√≥ thay ƒë·ªïi kh√¥ng
    if current_thinking != st.session_state.sync_flags["last_thinking_state"]:
        st.session_state.sync_flags["last_thinking_state"] = current_thinking
        
        if current_thinking:
            # L∆∞u temperature hi·ªán t·∫°i tr∆∞·ªõc khi set = 1.0
            if "saved_temperature" not in st.session_state:
                st.session_state.saved_temperature = st.session_state.model_settings["temperature"]
            st.session_state.model_settings["temperature"] = 1.0
        else:
            # Kh√¥i ph·ª•c temperature ƒë√£ l∆∞u
            if "saved_temperature" in st.session_state:
                st.session_state.model_settings["temperature"] = st.session_state.saved_temperature

def render_sidebar():
    """Render sidebar v·ªõi c√°c t√πy ch·ªçn c·∫•u h√¨nh"""
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # Model Selection
        st.subheader("ü§ñ Ch·ªçn Model")
        
        model_options = list(MODELS.keys())
        model_labels = [anthropic_handler.format_model_display(model) for model in model_options]
        
        selected_model_index = model_options.index(st.session_state.model_settings["model"]) if st.session_state.model_settings["model"] in model_options else 0
        
        selected_model = st.selectbox(
            "Model:",
            options=model_options,
            index=selected_model_index,
            format_func=lambda x: anthropic_handler.format_model_display(x),
            key="model_selector"
        )
        
        st.session_state.model_settings["model"] = selected_model
        
        # Model Information
        model_info = anthropic_handler.get_model_info(selected_model)
        if model_info:
            with st.expander("‚ÑπÔ∏è Th√¥ng tin Model", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**Input:** {model_info['price']['input']}")
                    st.write(f"**Context:** {model_info['context_window']}")
                with col2:
                    st.write(f"**Output:** {model_info['price']['output']}")
                    st.write(f"**Max Output:** {model_info['max_output']}")
        
        st.divider()
        
        # Parameters
        st.subheader("üéõÔ∏è Tham s·ªë")
        
        # Max Tokens
        max_tokens = st.number_input(
            "Max Tokens:",
            min_value=100,
            max_value=50000,
            value=st.session_state.model_settings["max_tokens"],
            step=100,
            help="S·ªë token t·ªëi ƒëa cho response"
        )
        st.session_state.model_settings["max_tokens"] = max_tokens
        
        # Temperature v·ªõi ƒë·ªìng b·ªô thinking mode
        thinking_enabled = model_info.get("extended_thinking", False) if model_info else False
        
        # Extended Thinking checkbox (ƒë·∫∑t tr∆∞·ªõc temperature ƒë·ªÉ x·ª≠ l√Ω ƒë·ªìng b·ªô)
        if thinking_enabled:
            enable_thinking = st.checkbox(
                "üß† B·∫≠t Extended Thinking",
                value=st.session_state.model_settings["thinking"],
                key="thinking_checkbox",
                help="Cho ph√©p model suy nghƒ© tr∆∞·ªõc khi tr·∫£ l·ªùi"
            )
            st.session_state.model_settings["thinking"] = enable_thinking
        else:
            st.session_state.model_settings["thinking"] = False
            
        current_thinking = st.session_state.model_settings["thinking"] and thinking_enabled
        
        # X·ª≠ l√Ω ƒë·ªìng b·ªô thinking v√† temperature
        handle_thinking_temperature_sync(thinking_enabled, current_thinking)
        
        if current_thinking:
            # Khi thinking enabled, temperature c·ªë ƒë·ªãnh = 1.0
            st.markdown("**Temperature:** *1.0 (c·ªë ƒë·ªãnh khi thinking enabled)*")
            st.info("‚ö†Ô∏è Temperature ƒë∆∞·ª£c c·ªë ƒë·ªãnh ·ªü 1.0 khi Extended Thinking ƒë∆∞·ª£c b·∫≠t")
            # Hi·ªÉn th·ªã temperature ƒë√£ l∆∞u
            if "saved_temperature" in st.session_state:
                st.caption(f"üíæ Temperature ƒë√£ l∆∞u: {st.session_state.saved_temperature}")
        else:
            # Cho ph√©p ƒëi·ªÅu ch·ªânh temperature
            temperature = st.number_input(
                "Temperature:",
                min_value=0.0,
                max_value=1.0,
                value=st.session_state.model_settings["temperature"],
                step=0.1,
                format="%.1f",
                help="M·ª©c ƒë·ªô s√°ng t·∫°o (0 = conservative, 1 = creative)"
            )
            st.session_state.model_settings["temperature"] = temperature
        
        # Extended Thinking Budget Tokens
        if thinking_enabled and st.session_state.model_settings["thinking"]:
            budget_tokens = st.number_input(
                "Budget Tokens cho Thinking:",
                min_value=1024,  # Minimum theo API
                max_value=100000,
                value=max(st.session_state.model_settings["budget_tokens"], 1024),
                step=1000,
                help="S·ªë tokens cho qu√° tr√¨nh thinking"
            )
            st.session_state.model_settings["budget_tokens"] = budget_tokens
            
            # Validation: budget_tokens ph·∫£i < max_tokens
            current_budget = st.session_state.model_settings["budget_tokens"]
            current_max = st.session_state.model_settings["max_tokens"]
            
            if current_budget >= current_max:
                st.error(f"‚ö†Ô∏è Budget tokens ({current_budget}) ph·∫£i nh·ªè h∆°n Max tokens ({current_max})")
                # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh
                adjusted_budget = max(1024, current_max - 1000)
                st.session_state.model_settings["budget_tokens"] = adjusted_budget
                st.info(f"‚úÖ ƒê√£ t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh Budget tokens th√†nh {adjusted_budget}")
                st.rerun()  # Refresh ƒë·ªÉ c·∫≠p nh·∫≠t UI
            
            # Warning n·∫øu qu√° g·∫ßn
            elif current_budget >= current_max * 0.8:
                st.warning("‚ö†Ô∏è Budget tokens qu√° g·∫ßn Max tokens, c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn ch·∫•t l∆∞·ª£ng response")
        
        if not thinking_enabled:
            st.info("‚ö†Ô∏è Model n√†y kh√¥ng h·ªó tr·ª£ Extended Thinking")
        
        st.divider()
        
        # System Prompt
        st.subheader("üìù System Prompt")
        system_prompt = st.text_area(
            "System Prompt:",
            value=st.session_state.model_settings["system_prompt"],
            height=100,
            placeholder="Nh·∫≠p system prompt ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng AI...",
            help="H∆∞·ªõng d·∫´n cho AI v·ªÅ c√°ch ph·∫£n h·ªìi"
        )
        st.session_state.model_settings["system_prompt"] = system_prompt
        
        st.divider()
        
        # Streaming
        st.subheader("üöÄ T√πy ch·ªçn kh√°c")
        use_streaming = st.checkbox(
            "B·∫≠t Streaming",
            value=st.session_state.model_settings["use_streaming"],
            help="Hi·ªÉn th·ªã response theo th·ªùi gian th·ª±c"
        )
        st.session_state.model_settings["use_streaming"] = use_streaming
        
        # Debug mode
        if DEBUG:
            st.subheader("üêõ Debug")
            if st.button("Hi·ªÉn th·ªã Session State"):
                st.json(dict(st.session_state))
        
        st.divider()
        
        # API Rules Information
        st.subheader("üìã Quy t·∫Øc API")
        with st.expander("‚ÑπÔ∏è Th√¥ng tin quan tr·ªçng", expanded=False):
            st.markdown("""
            **Extended Thinking Rules:**
            - Temperature ph·∫£i = 1.0 khi thinking enabled
            - Budget tokens ‚â• 1024 (minimum)
            - Max tokens > Budget tokens
            - Ch·ªâ model c√≥ h·ªó tr·ª£ m·ªõi c√≥ th·ªÉ d√πng thinking
            
            **Token Guidelines:**
            - S·ª≠ d·ª•ng input number ƒë·ªÉ nh·∫≠p gi√° tr·ªã ch√≠nh x√°c
            - H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh n·∫øu kh√¥ng h·ª£p l·ªá
            - Giao di·ªán t·ªëi ∆∞u ƒë·ªÉ gi·∫£m reload trang
            """)
        
        st.divider()
        
        # Chat Management
        st.subheader("üí¨ Qu·∫£n l√Ω Chat")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üóëÔ∏è X√≥a Chat", use_container_width=True):
                st.session_state.messages = []
                st.rerun()
        
        with col2:
            if st.button("üíæ L∆∞u Chat", use_container_width=True):
                save_chat_history()
        
        # Chat Statistics
        if st.session_state.messages:
            st.subheader("üìä Th·ªëng k√™")
            total_messages = len(st.session_state.messages)
            user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
            assistant_messages = len([m for m in st.session_state.messages if m["role"] == "assistant"])
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("T·ªïng tin nh·∫Øn", total_messages)
            with col2:
                st.metric("C·ªßa b·∫°n", user_messages)

def save_chat_history():
    """L∆∞u l·ªãch s·ª≠ chat"""
    if st.session_state.messages:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"chat_history_{timestamp}.json"
        
        chat_data = {
            "timestamp": timestamp,
            "model_settings": st.session_state.model_settings,
            "messages": st.session_state.messages
        }
        
        # Trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø, b·∫°n c√≥ th·ªÉ l∆∞u v√†o file ho·∫∑c database
        st.session_state.chat_history.append(chat_data)
        st.success(f"‚úÖ ƒê√£ l∆∞u chat history: {filename}")

def render_chat_interface():
    """Render giao di·ªán chat ch√≠nh"""
    st.title(f"{PAGE_ICON} {PAGE_TITLE}")
    
    # Hi·ªÉn th·ªã th√¥ng tin model hi·ªán t·∫°i
    current_model = st.session_state.model_settings["model"]
    model_display = anthropic_handler.format_model_display(current_model)
    
    st.info(f"ü§ñ ƒêang s·ª≠ d·ª•ng: **{model_display}** | "
           f"Streaming: {'‚úÖ' if st.session_state.model_settings['use_streaming'] else '‚ùå'} | "
           f"Thinking: {'üß†' if st.session_state.model_settings['thinking'] else '‚ùå'}")
    
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input t·ª´ user
    if prompt := st.chat_input(CHAT_INPUT_PLACEHOLDER):
        # Gi·ªõi h·∫°n ƒë·ªô d√†i l·ªãch s·ª≠
        if len(st.session_state.messages) >= MAX_HISTORY_LENGTH:
            st.session_state.messages = st.session_state.messages[-(MAX_HISTORY_LENGTH-2):]
        
        # Th√™m message c·ªßa user
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # T·∫°o response t·ª´ assistant
        with st.chat_message("assistant"):
            generate_response()

def sync_validated_parameters(validated_params):
    """ƒê·ªìng b·ªô parameters ƒë√£ ƒë∆∞·ª£c validate tr·ªü l·∫°i session state"""
    st.session_state.model_settings["max_tokens"] = validated_params["max_tokens"]
    st.session_state.model_settings["budget_tokens"] = validated_params["budget_tokens"] 
    st.session_state.model_settings["temperature"] = validated_params["temperature"]

def generate_response():
    """T·∫°o response t·ª´ AI"""
    settings = st.session_state.model_settings
    
    try:
        # Validate parameters tr∆∞·ªõc khi g·ªçi API
        validated = anthropic_handler.validate_and_fix_parameters(
            settings["model"],
            settings["max_tokens"],
            settings["budget_tokens"],
            settings["temperature"],
            settings["thinking"]
        )
        
        # ƒê·ªìng b·ªô parameters ƒë√£ ƒë∆∞·ª£c validate
        sync_validated_parameters(validated)
        
        # Hi·ªÉn th·ªã warnings n·∫øu c√≥
        if validated["warnings"]:
            warning_container = st.container()
            with warning_container:
                for warning in validated["warnings"]:
                    st.warning(f"‚ö†Ô∏è {warning}")
            time.sleep(1)  # Cho user ƒë·ªçc warnings
        
        if settings["use_streaming"]:
            # Streaming response
            response_placeholder = st.empty()
            full_response = ""
            
            with st.spinner("ü§î ƒêang suy nghƒ©..."):
                for chunk in anthropic_handler.stream_response(
                    model=settings["model"],
                    messages=st.session_state.messages,
                    system_prompt=settings["system_prompt"] if settings["system_prompt"].strip() else None,
                    max_tokens=validated["max_tokens"],
                    thinking=settings["thinking"],
                    budget_tokens=validated["budget_tokens"],
                    temperature=validated["temperature"]
                ):
                    full_response += chunk
                    # Th√™m cursor effect
                    response_placeholder.markdown(full_response + "‚ñå")
                    time.sleep(0.01)
            
            # Hi·ªÉn th·ªã response cu·ªëi c√πng
            response_placeholder.markdown(full_response)
            
        else:
            # Non-streaming response
            with st.spinner("ü§î ƒêang t·∫°o ph·∫£n h·ªìi..."):
                full_response = anthropic_handler.get_response(
                    model=settings["model"],
                    messages=st.session_state.messages,
                    system_prompt=settings["system_prompt"] if settings["system_prompt"].strip() else None,
                    max_tokens=validated["max_tokens"],
                    thinking=settings["thinking"],
                    budget_tokens=validated["budget_tokens"],
                    temperature=validated["temperature"]
                )
            st.markdown(full_response)
        
        # Th√™m response v√†o session state
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫°o response: {str(e)}")

def render_welcome_message():
    """Hi·ªÉn th·ªã th√¥ng b√°o ch√†o m·ª´ng khi ch∆∞a c√≥ tin nh·∫Øn"""
    if not st.session_state.messages:
        st.markdown("""
        ### üëã Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi Claude AI Chat!
        
        **H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**
        - ü§ñ Ch·ªçn model ph√π h·ª£p trong sidebar
        - üéõÔ∏è ƒêi·ªÅu ch·ªânh c√°c tham s·ªë theo nhu c·∫ßu (slider ho·∫∑c nh·∫≠p tr·ª±c ti·∫øp)
        - üß† B·∫≠t Extended Thinking ƒë·ªÉ xem qu√° tr√¨nh suy nghƒ© c·ªßa AI
        - üìù S·ª≠ d·ª•ng System Prompt ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng AI
        - üöÄ B·∫≠t Streaming ƒë·ªÉ xem response theo th·ªùi gian th·ª±c
        
        **T√≠nh nƒÉng ƒë∆∞·ª£c c·∫£i thi·ªán:**
        - ‚úÖ **Smart Validation**: T·ª± ƒë·ªông ki·ªÉm tra v√† ƒëi·ªÅu ch·ªânh parameters
        - üéöÔ∏è **Smart Temperature**: T·ª± ƒë·ªông c·ªë ƒë·ªãnh/kh√¥i ph·ª•c khi b·∫≠t/t·∫Øt thinking
        - üß† **Thinking Mode**: L∆∞u v√† kh√¥i ph·ª•c temperature khi chuy·ªÉn ƒë·ªïi
        - ‚ö° **Streamlined UI**: Ch·ªâ s·ª≠ d·ª•ng input ƒë·ªÉ tr√°nh reload nhi·ªÅu l·∫ßn
        - üìä **Visual Warnings**: Hi·ªÉn th·ªã c·∫£nh b√°o validation tr·ª±c quan
        
        **C√°ch s·ª≠ d·ª•ng:**
        - üî¢ **Input Fields**: Nh·∫≠p gi√° tr·ªã tr·ª±c ti·∫øp cho c√°c tham s·ªë
        - üíæ **Auto Save**: Temperature ƒë∆∞·ª£c l∆∞u khi b·∫≠t thinking v√† kh√¥i ph·ª•c khi t·∫Øt
        - ‚ö†Ô∏è **Smart Validation**: H·ªá th·ªëng t·ª± ƒë·ªông ki·ªÉm tra v√† ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn
        
        **B·∫Øt ƒë·∫ßu tr√≤ chuy·ªán b·∫±ng c√°ch nh·∫≠p tin nh·∫Øn ·ªü b√™n d∆∞·ªõi!** üëá
        
        *L∆∞u √Ω: H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh c√°c tham s·ªë ƒë·ªÉ tu√¢n th·ªß quy t·∫Øc API c·ªßa Anthropic. Giao di·ªán ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ gi·∫£m reload trang.*
        """)

def main():
    """H√†m ch√≠nh"""
    # Kh·ªüi t·∫°o session state
    initialize_session_state()
    
    # Render sidebar
    render_sidebar()
    
    # Render chat interface
    render_chat_interface()
    
    # Hi·ªÉn th·ªã welcome message n·∫øu ch∆∞a c√≥ tin nh·∫Øn
    render_welcome_message()

if __name__ == "__main__":
    main()