import anthropic
from typing import List, Dict, Any, Optional, Generator
import logging
from config import ANTHROPIC_API_KEY, DEBUG

# Configure logging
logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO)
logger = logging.getLogger(__name__)

# Model definitions
MODELS = {
    "claude-opus-4-20250514": {
        "can_reasoning": True,
        "extended_thinking": True,
        "price": {
            "input": "$15 / MTok",
            "output": "$75 / MTok",
        },
        "context_window": "200K",
        "max_output": "32000 tokens",
        "description": "Our most capable model",
        "display_name": "Claude Opus 4"
    },
    "claude-sonnet-4-20250514": {
        "can_reasoning": True,
        "extended_thinking": True,
        "price": {
            "input": "$3 / MTok",
            "output": "$15 / MTok",
        },
        "context_window": "200K",
        "max_output": "64000 tokens",
        "description": "High-performance model",
        "display_name": "Claude Sonnet 4"
    },
    "claude-3-7-sonnet-20250219": {
        "can_reasoning": True,
        "extended_thinking": True,
        "price": {
            "input": "$3 / MTok",
            "output": "$15 / MTok",
        },
        "context_window": "200K",
        "max_output": "64000 tokens",
        "description": "High-performance model with early extended thinking",
        "display_name": "Claude 3.7 Sonnet"
    },
    "claude-3-5-sonnet-20241022": {
        "can_reasoning": True,
        "extended_thinking": False,
        "price": {
            "input": "$3 / MTok",
            "output": "$15 / MTok",
        },
        "context_window": "200K",
        "max_output": "8192 tokens",
        "description": "Our previous intelligent model",
        "display_name": "Claude 3.5 Sonnet"
    },
    "claude-3-5-haiku-20241022": {
        "can_reasoning": False,
        "extended_thinking": False,
        "price": {
            "input": "$0.80 / MTok",
            "output": "$4 / MTok",
        },
        "context_window": "200K",
        "max_output": "8192 tokens",
        "description": "Our fastest model",
        "display_name": "Claude 3.5 Haiku"
    },
    "claude-3-opus-20240229": {
        "can_reasoning": True,
        "extended_thinking": False,
        "price": {
            "input": "$15 / MTok",
            "output": "$75 / MTok",
        },
        "context_window": "200K",
        "max_output": "4096 tokens",
        "description": "Powerful model for complex tasks",
        "display_name": "Claude 3 Opus"
    },
    "claude-3-haiku-20240307": {
        "can_reasoning": False,
        "extended_thinking": False,
        "price": {
            "input": "$0.25 / MTok",
            "output": "$1.25 / MTok",
        },
        "context_window": "200K",
        "max_output": "4096 tokens",
        "description": "Fast and compact model for near-instant responsiveness",
        "display_name": "Claude 3 Haiku"
    }
}

class AnthropicHandler:
    """Handler cho Anthropic API vá»›i streaming support"""
    
    def __init__(self, api_key: str = ANTHROPIC_API_KEY):
        """
        Khá»Ÿi táº¡o Anthropic client
        
        Args:
            api_key: Anthropic API key
        """
        if not api_key:
            raise ValueError("API key khÃ´ng Ä‘Æ°á»£c cung cáº¥p")
        
        self.client = anthropic.Anthropic(api_key=api_key)
        logger.info("Anthropic client Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o")
    
    def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """
        Láº¥y danh sÃ¡ch cÃ¡c model kháº£ dá»¥ng
        
        Returns:
            Dictionary chá»©a thÃ´ng tin cÃ¡c model
        """
        return MODELS
    
    def get_model_info(self, model_id: str) -> Dict[str, Any]:
        """
        Láº¥y thÃ´ng tin cá»§a má»™t model cá»¥ thá»ƒ
        
        Args:
            model_id: ID cá»§a model
            
        Returns:
            Dictionary chá»©a thÃ´ng tin model
        """
        return MODELS.get(model_id, {})
    
    def validate_model_features(self, model_id: str, thinking: bool = False) -> bool:
        """
        Kiá»ƒm tra xem model cÃ³ há»— trá»£ cÃ¡c tÃ­nh nÄƒng Ä‘Æ°á»£c yÃªu cáº§u khÃ´ng
        
        Args:
            model_id: ID cá»§a model
            thinking: CÃ³ cáº§n extended thinking khÃ´ng
            
        Returns:
            True náº¿u model há»— trá»£, False náº¿u khÃ´ng
        """
        model_info = self.get_model_info(model_id)
        if not model_info:
            return False
        
        if thinking and not model_info.get("extended_thinking", False):
            return False
            
        return True
    
    def validate_and_fix_parameters(
        self,
        model: str,
        max_tokens: int,
        budget_tokens: int,
        temperature: float,
        thinking: bool
    ) -> Dict[str, Any]:
        """
        Validate vÃ  tá»± Ä‘á»™ng sá»­a cÃ¡c parameters Ä‘á»ƒ trÃ¡nh lá»—i API
        
        Args:
            model: Model ID
            max_tokens: Sá»‘ token tá»‘i Ä‘a
            budget_tokens: Budget tokens cho thinking
            temperature: Temperature
            thinking: CÃ³ báº­t thinking khÃ´ng
            
        Returns:
            Dictionary chá»©a parameters Ä‘Ã£ Ä‘Æ°á»£c validate vÃ  sá»­a
        """
        warnings = []
        
        # Kiá»ƒm tra vÃ  sá»­a budget_tokens minimum
        if thinking and budget_tokens < 1024:
            budget_tokens = 1024
            warnings.append("Budget tokens Ä‘Ã£ Ä‘Æ°á»£c tÄƒng lÃªn 1024 (minimum required)")
        
        # Kiá»ƒm tra vÃ  sá»­a max_tokens vs budget_tokens
        if thinking and max_tokens <= budget_tokens:
            max_tokens = budget_tokens + 1000  # Äáº£m báº£o cÃ³ khoáº£ng cÃ¡ch an toÃ n
            warnings.append(f"Max tokens Ä‘Ã£ Ä‘Æ°á»£c tÄƒng lÃªn {max_tokens} Ä‘á»ƒ lá»›n hÆ¡n budget_tokens")
        
        # Kiá»ƒm tra vÃ  sá»­a temperature khi thinking enabled
        if thinking:
            # Theo documentation, temperature pháº£i = 1 khi thinking enabled
            if temperature != 1.0:
                temperature = 1.0
                warnings.append("Temperature Ä‘Ã£ Ä‘Æ°á»£c set = 1.0 (required when thinking enabled)")
        
        return {
            "max_tokens": max_tokens,
            "budget_tokens": budget_tokens,
            "temperature": temperature,
            "warnings": warnings
        }
    
    def _build_request_params(
        self,
        model: str,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        max_tokens: int = 1024,
        thinking: bool = False,
        budget_tokens: int = 10000,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        XÃ¢y dá»±ng parameters cho API request
        
        Args:
            model: Model ID
            messages: Danh sÃ¡ch tin nháº¯n
            system_prompt: System prompt (tÃ¹y chá»n)
            max_tokens: Sá»‘ token tá»‘i Ä‘a
            thinking: Báº­t extended thinking
            budget_tokens: Budget tokens cho thinking
            temperature: Temperature cho response
            
        Returns:
            Dictionary chá»©a parameters
        """
        # Validate vÃ  sá»­a parameters
        validated = self.validate_and_fix_parameters(
            model, max_tokens, budget_tokens, temperature, thinking
        )
        
        # Log warnings náº¿u cÃ³
        for warning in validated["warnings"]:
            logger.warning(warning)
        
        params = {
            "model": model,
            "max_tokens": validated["max_tokens"],
            "messages": messages,
            "temperature": validated["temperature"]
        }
        
        if system_prompt and system_prompt.strip():
            params["system"] = system_prompt
            
        if thinking and self.validate_model_features(model, thinking=True):
            params["thinking"] = {
                "type": "enabled",
                "budget_tokens": validated["budget_tokens"]
            }
            logger.debug(f"Extended thinking enabled vá»›i {validated['budget_tokens']} budget tokens")
            
        return params
    
    def get_response(
        self,
        model: str,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        max_tokens: int = 1024,
        thinking: bool = False,
        budget_tokens: int = 10000,
        temperature: float = 0.7
    ) -> str:
        """
        Láº¥y response tá»« Anthropic API (khÃ´ng streaming)
        
        Args:
            model: Model ID
            messages: Danh sÃ¡ch tin nháº¯n
            system_prompt: System prompt (tÃ¹y chá»n)
            max_tokens: Sá»‘ token tá»‘i Ä‘a
            thinking: Báº­t extended thinking
            budget_tokens: Budget tokens cho thinking
            temperature: Temperature
            
        Returns:
            Response text
        """
        try:
            params = self._build_request_params(
                model, messages, system_prompt, max_tokens, 
                thinking, budget_tokens, temperature
            )
            
            logger.debug(f"Gá»i API vá»›i model: {model}, thinking: {thinking}")
            response = self.client.messages.create(**params)
            
            # Xá»­ lÃ½ response cÃ³ thinking
            if thinking and hasattr(response, 'content'):
                full_response = ""
                for block in response.content:
                    if block.type == "thinking":
                        full_response += f"\n**ðŸ¤” Thinking:** {block.thinking}\n\n"
                    elif block.type == "text":
                        full_response += block.text
                return full_response
            else:
                return response.content[0].text
                
        except anthropic.APIError as e:
            logger.error(f"Anthropic API error: {str(e)}")
            return f"âŒ Lá»—i API: {str(e)}"
        except Exception as e:
            logger.error(f"Unexpected error: {str(e)}")
            return f"âŒ Lá»—i khÃ´ng mong muá»‘n: {str(e)}"
    
    def stream_response(
        self,
        model: str,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        max_tokens: int = 1024,
        thinking: bool = False,
        budget_tokens: int = 10000,
        temperature: float = 0.7
    ) -> Generator[str, None, None]:
        """
        Stream response tá»« Anthropic API
        
        Args:
            model: Model ID
            messages: Danh sÃ¡ch tin nháº¯n
            system_prompt: System prompt (tÃ¹y chá»n)
            max_tokens: Sá»‘ token tá»‘i Ä‘a
            thinking: Báº­t extended thinking
            budget_tokens: Budget tokens cho thinking
            temperature: Temperature
            
        Yields:
            Tá»«ng chunk cá»§a response
        """
        try:
            params = self._build_request_params(
                model, messages, system_prompt, max_tokens, 
                thinking, budget_tokens, temperature
            )
            
            logger.debug(f"Streaming vá»›i model: {model}, thinking: {thinking}")
            
            with self.client.messages.stream(**params) as stream:
                for event in stream:
                    if event.type == "content_block_delta":
                        if hasattr(event.delta, 'text'):
                            yield event.delta.text
                    elif event.type == "thinking_block_delta":
                        if hasattr(event.delta, 'thinking'):
                            yield f"\n**ðŸ¤” [Thinking]** {event.delta.thinking}"
                    elif event.type == "content_block_start":
                        if hasattr(event.content_block, 'type') and event.content_block.type == "thinking":
                            yield "\n\n**ðŸ¤” Thinking process:**\n"
                            
        except anthropic.APIError as e:
            logger.error(f"Anthropic API streaming error: {str(e)}")
            yield f"âŒ Lá»—i API streaming: {str(e)}"
        except Exception as e:
            logger.error(f"Unexpected streaming error: {str(e)}")
            yield f"âŒ Lá»—i streaming khÃ´ng mong muá»‘n: {str(e)}"
    
    def estimate_tokens(self, text: str) -> int:
        """
        Æ¯á»›c tÃ­nh sá»‘ tokens trong text (xáº¥p xá»‰)
        
        Args:
            text: Text cáº§n Æ°á»›c tÃ­nh
            
        Returns:
            Sá»‘ tokens Æ°á»›c tÃ­nh
        """
        # Æ¯á»›c tÃ­nh Ä‘Æ¡n giáº£n: 1 token â‰ˆ 4 characters
        return len(text) // 4
    
    def format_model_display(self, model_id: str) -> str:
        """
        Format tÃªn model Ä‘á»ƒ hiá»ƒn thá»‹ trong UI
        
        Args:
            model_id: ID cá»§a model
            
        Returns:
            TÃªn hiá»ƒn thá»‹ cá»§a model
        """
        model_info = self.get_model_info(model_id)
        if not model_info:
            return model_id
        
        display_name = model_info.get("display_name", model_id)
        description = model_info.get("description", "")
        
        return f"{display_name} - {description}"

# Instance máº·c Ä‘á»‹nh Ä‘á»ƒ sá»­ dá»¥ng
anthropic_handler = AnthropicHandler()